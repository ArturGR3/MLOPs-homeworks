{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import kaggle\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi \n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/home/artur/mlops-zoomcamp/notebooks/project/mlruns/1', creation_time=1715770587504, experiment_id='1', last_update_time=1715770587504, lifecycle_stage='active', name='Kaggle_Competition_Example', tags={}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define MLflow experiment name\n",
    "experiment_name = \"Kaggle_Competition_Example\"\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(comp_name: str) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Load data from a Kaggle competition zip file.\n",
    "    Parameters:\n",
    "        comp_name (str): Name of the Kaggle competition.\n",
    "    Returns:\n",
    "        tuple: A tuple containing three DataFrames: train, test, and submission.\n",
    "    \"\"\"\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "\n",
    "    # Download the competition files\n",
    "    api.competition_download_files(comp_name, path='.', force=True)\n",
    "\n",
    "    # Unzip the downloaded files\n",
    "    with zipfile.ZipFile(f\"{comp_name}.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(\".\")\n",
    "\n",
    "    # Load data into DataFrames\n",
    "    submission = pd.read_csv(\"sample_submission.csv\")\n",
    "    test = pd.read_csv(\"test.csv\")\n",
    "    train = pd.read_csv(\"train.csv\")\n",
    "\n",
    "    return train, test, submission\n",
    "\n",
    "def adjust_dtypes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adjust data types for the DataFrame columns.\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to adjust.\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with adjusted data types.\n",
    "    \"\"\"\n",
    "    int_columns = df.select_dtypes(include=['int64']).columns\n",
    "    float_columns = df.select_dtypes(include=['float64']).columns\n",
    "    \n",
    "    # Change integer columns to the smallest type that fits the data\n",
    "    for col in int_columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    \n",
    "    # Change float columns to the smallest float type that fits the data\n",
    "    for col in float_columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data\n",
    "train, test, submission = load_data('playground-series-s4e5')\n",
    "\n",
    "# Adjust data types\n",
    "train = adjust_dtypes(train)\n",
    "test = adjust_dtypes(test)\n",
    "submission = adjust_dtypes(submission)\n",
    "\n",
    "# Split data into features and target\n",
    "x_train_full = train.drop(columns=['FloodProbability'])\n",
    "y_train_full = train['FloodProbability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and validation sets\n",
    "train_df, val_df = train_test_split(train, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train = train_df.drop(columns=['FloodProbability'])\n",
    "y_train = train_df['FloodProbability']\n",
    "\n",
    "x_val = val_df.drop(columns=['FloodProbability'])\n",
    "y_val = val_df['FloodProbability']\n",
    "\n",
    "x_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240518_122412\"\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Dynamic stacking is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "Detecting stacked overfitting by sub-fitting AutoGluon on the input data. That is, copies of AutoGluon will be sub-fit on subset(s) of the data. Then, the holdout validation data is used to detect stacked overfitting.\n",
      "Sub-fit(s) time limit is: 600 seconds.\n",
      "Starting holdout-based sub-fit for dynamic stacking. Context path is: AutogluonModels/ag-20240518_122412/ds_sub_fit/sub_fit_ho.\n",
      "2024-05-18 12:24:12,684\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "Running the sub-fit in a ray process to avoid memory leakage.\n",
      "Spend 180 seconds for the sub-fit(s) during dynamic stacking.\n",
      "Time left for full fit of AutoGluon: 420 seconds.\n",
      "Starting full fit now with num_stack_levels 1.\n",
      "Beginning AutoGluon training ... Time limit = 420s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240518_122412\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.9.19\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #67~20.04.1-Ubuntu SMP Thu Apr 18 14:26:18 UTC 2024\n",
      "CPU Count:          4\n",
      "Memory Avail:       11.38 GB / 15.61 GB (72.9%)\n",
      "Disk Space Avail:   13.04 GB / 48.28 GB (27.0%)\n",
      "===================================================\n",
      "Train Data Rows:    894365\n",
      "Train Data Columns: 21\n",
      "Label Column:       FloodProbability\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    11618.34 MB\n",
      "\tTrain Data (Original)  Memory Usage: 20.47 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', []) : 21 | ['id', 'MonsoonIntensity', 'TopographyDrainage', 'RiverManagement', 'Deforestation', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('int', []) : 21 | ['id', 'MonsoonIntensity', 'TopographyDrainage', 'RiverManagement', 'Deforestation', ...]\n",
      "\t1.7s = Fit runtime\n",
      "\t21 features in original data used to generate 21 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 20.47 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 1.83s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 108 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 278.71s of the 418.15s of remaining time.\n",
      "\tNot enough time to generate out-of-fold predictions for model. Estimated time required was 2911.61s compared to 361.58s of available time.\n",
      "\tTime limit exceeded... Skipping KNeighborsUnif_BAG_L1.\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 275.81s of the 415.25s of remaining time.\n",
      "\tNot enough time to generate out-of-fold predictions for model. Estimated time required was 2463.55s compared to 357.82s of available time.\n",
      "\tTime limit exceeded... Skipping KNeighborsDist_BAG_L1.\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 273.24s of the 412.68s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=1.54%)\n",
      "\t0.8374\t = Validation score   (r2)\n",
      "\t237.38s\t = Training   runtime\n",
      "\t95.66s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 20.5s of the 159.94s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=1.58%)\n",
      "\t0.1766\t = Validation score   (r2)\n",
      "\t22.87s\t = Training   runtime\n",
      "\t2.57s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 133.02s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t0.8374\t = Validation score   (r2)\n",
      "\t1.19s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting 106 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 131.76s of the 131.7s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=1.92%)\n",
      "\t0.8484\t = Validation score   (r2)\n",
      "\t116.3s\t = Training   runtime\n",
      "\t25.53s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 8.65s of the 8.59s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=1.92%)\n",
      "\t0.0825\t = Validation score   (r2)\n",
      "\t13.66s\t = Training   runtime\n",
      "\t0.52s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the -9.64s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L2': 0.889, 'LightGBMXT_BAG_L1': 0.111}\n",
      "\t0.8486\t = Validation score   (r2)\n",
      "\t1.76s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 431.58s ... Best model: \"WeightedEnsemble_L3\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240518_122412\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<autogluon.tabular.predictor.predictor.TabularPredictor at 0x7f69e5385f70>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AutoML parameters\n",
    "predictor = TabularPredictor(label='FloodProbability', eval_metric='r2')\n",
    "predictor.fit(train_data=train_df, time_limit=600, presets='best_quality')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(223592,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get out-of-fold predictions and calculate oof R^2 score\n",
    "y_val_predict = predictor.predict(x_val)\n",
    "r2_score = r2_score(y_val, y_val_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-fold R^2 score: 0.85515524966557\n"
     ]
    }
   ],
   "source": [
    "print(\"Out-of-fold R^2 score:\", r2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get out-of-fold predictions and calculate oof R^2 score\n",
    "y_val_predict = predictor.predic['oof_pred']\n",
    "oof_r2 = r2_score(y_train, y_oof_pred)\n",
    "print(\"Out-of-fold R^2 score:\", oof_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:42:23] Stdout logging level is INFO.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:42:23] Task: reg\n",
      "\n",
      "[11:42:23] Start automl preset with listed constraints:\n",
      "[11:42:23] - time: 600.00 seconds\n",
      "[11:42:23] - CPU: 4 cores\n",
      "[11:42:23] - memory: 16 GB\n",
      "\n",
      "[11:42:23] \u001b[1mTrain data shape: (894365, 22)\u001b[0m\n",
      "\n",
      "[11:42:32] Layer \u001b[1m1\u001b[0m train process start. Time left 591.60 secs\n",
      "[11:42:33] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m ...\n",
      "[11:42:34] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m finished. score = \u001b[1m0.8448548330350316\u001b[0m\n",
      "[11:42:34] \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m fitting and predicting completed\n",
      "[11:42:34] Time left 588.68 secs\n",
      "\n",
      "[11:45:12] \u001b[1mSelector_LightGBM\u001b[0m fitting and predicting completed\n",
      "[11:45:12] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m ...\n",
      "[11:47:54] Time limit exceeded after calculating fold 0\n",
      "\n",
      "[11:47:54] Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m0.8396970290102965\u001b[0m\n",
      "[11:47:54] \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
      "[11:47:54] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m ...\n",
      "[11:49:07] Time limit exceeded after calculating fold 0\n",
      "\n",
      "[11:49:07] Fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m finished. score = \u001b[1m0.8462900706145828\u001b[0m\n",
      "[11:49:07] \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m fitting and predicting completed\n",
      "[11:49:07] Time left 196.04 secs\n",
      "\n",
      "[11:49:07] Time limit exceeded in one of the tasks. AutoML will blend level 1 models.\n",
      "\n",
      "[11:49:07] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[11:49:07] Blending: optimization starts with equal weights and score \u001b[1m0.8453643036187497\u001b[0m\n",
      "[11:49:08] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m0.8453981243596627\u001b[0m, weights = \u001b[1m[0.32253727 0.19474775 0.48271492]\u001b[0m\n",
      "[11:49:09] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m0.8454323552230888\u001b[0m, weights = \u001b[1m[0.11224213 0.23199542 0.65576243]\u001b[0m\n",
      "[11:49:10] Blending: iteration \u001b[1m2\u001b[0m: score = \u001b[1m0.8454338909126771\u001b[0m, weights = \u001b[1m[0.06962871 0.23397376 0.69639754]\u001b[0m\n",
      "[11:49:11] Blending: iteration \u001b[1m3\u001b[0m: score = \u001b[1m0.8469700741275916\u001b[0m, weights = \u001b[1m[0.         0.23415135 0.76584864]\u001b[0m\n",
      "[11:49:13] Blending: iteration \u001b[1m4\u001b[0m: score = \u001b[1m0.8469700741275916\u001b[0m, weights = \u001b[1m[0.         0.23415135 0.76584864]\u001b[0m\n",
      "[11:49:13] Blending: no score update. Terminated\n",
      "\n",
      "[11:49:13] \u001b[1mAutoml preset training completed in 409.41 seconds\u001b[0m\n",
      "\n",
      "[11:49:13] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.23415 * (1 averaged models Lvl_0_Pipe_1_Mod_0_LightGBM) +\n",
      "\t 0.76585 * (1 averaged models Lvl_0_Pipe_1_Mod_2_CatBoost) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# AutoML parameters\n",
    "torch.set_num_threads(4)\n",
    "automl_params = {\n",
    "    'task': Task('reg', loss = 'mse', metric = 'r2'),\n",
    "    'timeout': 10*60,\n",
    "    'reader_params': {\"n_jobs\": 4, 'cv': 3, 'random_state': 42},\n",
    "    'cpu_limit': 4\n",
    "}\n",
    "model = TabularAutoML(**automl_params)\n",
    "oof= model.fit_predict(train_df, roles= {'target': 'FloodProbability'}, verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models and record R2 scores\n",
    "models = {\n",
    "    \"Linear_Regression\": LinearRegression(),\n",
    "    \"Ridge\": Ridge(),\n",
    "    \"LightGBM\": lightgbm.LGBMRegressor(),\n",
    "    \"LightAutoML\": TabularAutoML(**automl_params)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6 m \n",
    "5 m 40  int16\n",
    "5 m 32  int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8452595986219906"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_train,oof.data[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val = model.predict(x_val)\n",
    "r2_val_before_tuning = r2_score(y_val, y_pred_val.data[:,0])\n",
    "\n",
    "model.fit_predict(train, roles= {'target': 'FloodProbability'})\n",
    "y_pred_test = model.predict(x_test).data[:,0]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, model_name):\n",
    "    \n",
    "    if model_name =='Ridge':\n",
    "        params = {'alpha': trial.suggest_float(\"alpha\", 0.01,10.0, log = True)}\n",
    "        model = Ridge(**params)\n",
    "    elif model_name == \"LightGBM\":\n",
    "        params = {\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 128),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, log = True),\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000)\n",
    "        }   \n",
    "        model = lightgbm.LGBMRegressor(**params)\n",
    "    else:\n",
    "        model = LinearRegression()\n",
    "        params = {}\n",
    "    \n",
    "    model.fit(x_train, y_train)     \n",
    "    y_pred_val = model.predict(x_val)\n",
    "    r2_val = r2_score(y_val, y_pred_val)\n",
    "    \n",
    "    # Start a nested run for the trial\n",
    "    with mlflow.start_run(run_name=f\"trial_{trial.number}\", nested=True) as trial_run:\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metric(\"R2 Score\", round(r2_val,4))\n",
    "\n",
    "    return r2_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logging_callback(study, trial):\n",
    "    \"\"\"\n",
    "    Callback function for logging Optuna trial results to MLflow.\n",
    "\n",
    "    Parameters:\n",
    "        study (optuna.study.Study): The study object.\n",
    "        trial (optuna.trial.Trial): The trial object.\n",
    "    \"\"\"\n",
    "    mlflow.log_params(trial.params)\n",
    "    mlflow.log_metric(f\"R2_Score_Trial_{trial.number}\", trial.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(4)\n",
    "\n",
    "with mlflow.start_run(run_name=\"50 trials vs 10 min LightAutoML\") as parent_run:\n",
    "    for model_name, model in models.items():\n",
    "        with mlflow.start_run(run_name=model_name, nested=True) as child_run:\n",
    "            # Fit the model before hyperparameter tuning\n",
    "            if model_name in [\"Linear_Regression\", \"Ridge\", \"LightGBM\"]:\n",
    "                \n",
    "                model.fit(x_train, y_train)\n",
    "                y_pred_val = model.predict(x_val)\n",
    "                r2_val_before_tuning = r2_score(y_val, y_pred_val)\n",
    "                r2_val_after_tuning = r2_val_before_tuning # for models that are not going through tuning\n",
    "\n",
    "                # Hyperparameter tuning with Optuna \n",
    "                if model_name in [\"Ridge\", \"LightGBM\"]:\n",
    "                    study = optuna.create_study(direction=\"maximize\", sampler=TPESampler())\n",
    "                    study.optimize(lambda trial: objective(trial, model_name), n_trials=50)\n",
    "                    best_params = study.best_params\n",
    "                    r2_val_after_tuning  = study.best_value\n",
    "                \n",
    "                # Train with the best hyperparameters\n",
    "                if model_name == \"Ridge\":\n",
    "                    best_model = Ridge(**best_params)\n",
    "                elif model_name == \"LightGBM\":\n",
    "                    best_model = lightgbm.LGBMRegressor(**best_params)\n",
    "                else:\n",
    "                    best_model = model\n",
    "                \n",
    "                best_model.fit(x_train_full, y_train_full)\n",
    "                y_pred_test = best_model.predict(x_test)    \n",
    "                \n",
    "            elif model_name == 'LightAutoML': \n",
    "                model.fit_predict(train_df, roles= {'target': 'FloodProbability'})\n",
    "                y_pred_val = model.predict(x_val)\n",
    "                r2_val_before_tuning = r2_score(y_val, y_pred_val.data[:,0])\n",
    "                \n",
    "                model.fit_predict(train, roles= {'target': 'FloodProbability'})\n",
    "                y_pred_test = model.predict(x_test).data[:,0]     \n",
    "                                \n",
    "            # Log R^2 score before tuning\n",
    "            mlflow.log_metric(\"0.R2 Score Pre Tuning\", round(r2_val_before_tuning,4), step=0)\n",
    "            if model_name != 'LightAutoML':\n",
    "                mlflow.log_params(best_params)\n",
    "                mlflow.log_metric(\"1.R2 Score Post Tuning\", round(r2_val_after_tuning,4), step=1)\n",
    "            else: \n",
    "                mlflow.log_metric(\"1.R2 Score Post Tuning\", round(r2_val_before_tuning,4), step=1)\n",
    "                          \n",
    "            # Update submission DataFrame with predictions\n",
    "            submission['FloodProbability'] = y_pred_test\n",
    "\n",
    "            # Save submission to a CSV file\n",
    "            submission_file = f\"submission_{model_name}.csv\"\n",
    "            submission.to_csv(submission_file, index=False)\n",
    "            \n",
    "            # Submit to Kaggle competition\n",
    "            competition_name = \"playground-series-s4e5\"\n",
    "            submission_message = f\"Submission with {model_name} Initial\"\n",
    "            # kaggle.api.competition_submit(submission_file, submission_message, competition_name)\n",
    "\n",
    "            # # # Get Kaggle submission score\n",
    "            submissions = kaggle.api.competitions_submissions_list(competition_name)\n",
    "            submission_score = None\n",
    "            for subs in submissions:\n",
    "                if subs['description'] == submission_message:\n",
    "                    submission_score = round(np.float64(subs['publicScore']),4)\n",
    "                    break\n",
    "\n",
    "            # Log parameters and artifacts\n",
    "            mlflow.log_param(\"Model Name\", model_name)\n",
    "            mlflow.log_param(\"Competition Name\", competition_name)\n",
    "            mlflow.log_param(\"Submission File\", submission_file)\n",
    "            mlflow.log_param(\"Submission Message\", submission_message)\n",
    "            # mlflow.log_metric(\"Kaggle Score\", submission_score, step=0)\n",
    "            mlflow.log_artifact(submission_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kagl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
